{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # Don't use this other than for matplotlib\n",
    "from numpy import array\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True,\n",
    "                                              transform=transform)\n",
    "indices = list(range(len(train_set)))\n",
    "np.random.shuffle(indices)\n",
    "train_after_split = SubsetRandomSampler(indices[:50000])\n",
    "validation_after_split = SubsetRandomSampler(indices[50000:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, sampler=train_after_split, batch_size=4)\n",
    "validation_loader = torch.utils.data.DataLoader(train_set, sampler=validation_after_split, batch_size=4)\n",
    "\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, \n",
    "                                             train=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = (\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image batch shape is torch.Size([4, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dress   Bag Dress Dress\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"Our image batch shape is\", images.size())\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "print(\" \".join(\"%5s\" % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Noise is added by removing one or two quadrants of the image\n",
    "# def noise_old(x):\n",
    "#     #temp = x\n",
    "#     for k in range(0,2):\n",
    "#         #print(\"Iteration \", k)\n",
    "#         x_offset = random.choice([0,14])\n",
    "#         y_offset = random.choice([0,14])\n",
    "#         #print(\"X: \", x_offset, \" Y: \", y_offset)\n",
    "#         for i in range(x_offset, x_offset+14):\n",
    "#             for j in range(y_offset, y_offset+14):\n",
    "#                 x[i][j] = 0\n",
    "#     return x\n",
    "\n",
    "def noise(x):\n",
    "    #print(type(x))\n",
    "    x = array(x)\n",
    "    for k in range(0,2):\n",
    "        #print(\"Iteration \", k)\n",
    "        x_offset = random.choice([0,14])\n",
    "        for i in range(x_offset, x_offset+14):\n",
    "            x[x_offset:x_offset+14,x_offset:x_offset+14] = torch.zeros(x[x_offset:x_offset+14,x_offset:x_offset+14].shape)\n",
    "    x = torch.from_numpy(x)\n",
    "    #print(type(x))\n",
    "    return x\n",
    "            \n",
    "# Apply noise to a set of images\n",
    "def apply_noise(imgs):\n",
    "    noisy_imgs = copy.deepcopy(imgs)\n",
    "            \n",
    "    for i in range(0, len(imgs)):\n",
    "        noisy_imgs[i][0] = noise(imgs[i][0])\n",
    "\n",
    "    imgs = imgs.view(imgs.size(0), -1)\n",
    "    noisy_imgs = noisy_imgs.view(noisy_imgs.size(0), -1)\n",
    "    \n",
    "    return imgs, noisy_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test noise function\n",
    "# arr = [[1 for i in range(28)] for j in range(28)]\n",
    "# arr2 = [[1 for i in range(28)] for j in range(28)]\n",
    "# big = [arr, arr2]\n",
    "# apply_noise(big)\n",
    "# for x in big:\n",
    "#     print(np.matrix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 32\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, z_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        self.encoder = nn.Linear(28*28, z_size)\n",
    "        self.decoder = nn.Linear(z_size, 28*28)\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        encoded = torch.sigmoid(self.encoder(input_x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(z_size)\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train():\n",
    "    start_training_time = datetime.datetime.now()\n",
    "    epochs = 20\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for train_data in train_loader:\n",
    "            train_imgs, _ = train_data\n",
    "            train_imgs, train_noisy_imgs = apply_noise(train_imgs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_output = autoencoder(train_noisy_imgs)\n",
    "            loss_func = mse_loss(train_output, train_imgs)\n",
    "            loss_func.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss_func.item()*train_imgs.size(0)\n",
    "            \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "            ))\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for validation_data in validation_loader:\n",
    "                val_imgs, _ = validation_data\n",
    "                val_imgs, val_noisy_imgs = apply_noise(val_imgs)\n",
    "                \n",
    "                val_output = autoencoder(val_noisy_imgs)\n",
    "                loss_func = mse_loss(val_output, val_imgs)\n",
    "                val_loss += loss_func.item()*val_imgs.size(0)\n",
    "        \n",
    "        val_loss = val_loss/len(validation_loader)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print('Epoch: {} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            val_loss\n",
    "            ))\n",
    "        \n",
    "    end_training_time = datetime.datetime.now()\n",
    "    diff_training_time = end_training_time - start_training_time\n",
    "    print(diff_training_time)\n",
    "    \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(loss_training, loss_type):\n",
    "    if loss_type is \"training\":\n",
    "        plot_label = 'training set MSE loss'\n",
    "    elif loss_type is \"validation\":\n",
    "        plot_label = 'validation set MSE loss'\n",
    "    plt.axis([0, 20, min(loss_training), max(loss_training)])\n",
    "    plt.plot(loss_training, label=plot_label)\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(train_loss, \"training\")\n",
    "loss_plot(val_loss, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_iter = iter(test_loader)\n",
    "    for i in range(8):\n",
    "        test_imgs, _ = test_iter.next()\n",
    "        test_imgs, test_noisy_imgs = apply_noise(test_imgs)\n",
    "\n",
    "        output = autoencoder(test_noisy_imgs)\n",
    "        test_noisy_imgs = test_noisy_imgs.view(4, 1, 28, 28)\n",
    "        test_noisy_imgs = test_noisy_imgs.detach().numpy()\n",
    "        output = output.view(4, 1, 28, 28)\n",
    "        output = output.detach().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(10,4))\n",
    "        for test_noisy_imgs, row in zip([test_noisy_imgs, output], axes):\n",
    "            for img, ax in zip(test_noisy_imgs, row):\n",
    "                ax.imshow(np.squeeze(img), cmap='gray')\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
