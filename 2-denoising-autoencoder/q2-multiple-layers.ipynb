{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # Don't use this other than for matplotlib\n",
    "from numpy import array\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, ), (0.5, ))])\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True,\n",
    "                                              transform=transform)\n",
    "indices = list(range(len(train_set)))\n",
    "np.random.shuffle(indices)\n",
    "train_after_split = SubsetRandomSampler(indices[:50000])\n",
    "validation_after_split = SubsetRandomSampler(indices[50000:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, sampler=train_after_split, batch_size=4)\n",
    "validation_loader = torch.utils.data.DataLoader(train_set, sampler=validation_after_split, batch_size=4)\n",
    "\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, \n",
    "                                             train=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = (\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "           \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image batch shape is torch.Size([4, 1, 28, 28])\n",
      "tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0., -1.,  ..., -1.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0., -1.,  ..., -1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0., -1.,  ..., -1.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0., -1.,  ..., -1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0., -1.,  ..., -1.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0., -1.,  ..., -1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\n",
      "tensor([[[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
      "         ...,\n",
      "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
      "         ...,\n",
      "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
      "\n",
      "        [[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
      "         ...,\n",
      "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
      "         [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-shirt/Top  Coat  Coat Pullover\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"Our image batch shape is\", images.size())\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "print(\" \".join(\"%5s\" % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Noise is added by removing one or two quadrants of the image\n",
    "# def noise_old(x):\n",
    "#     #temp = x\n",
    "#     for k in range(0,2):\n",
    "#         #print(\"Iteration \", k)\n",
    "#         x_offset = random.choice([0,14])\n",
    "#         y_offset = random.choice([0,14])\n",
    "#         #print(\"X: \", x_offset, \" Y: \", y_offset)\n",
    "#         for i in range(x_offset, x_offset+14):\n",
    "#             for j in range(y_offset, y_offset+14):\n",
    "#                 x[i][j] = 0\n",
    "#     return x\n",
    "\n",
    "def noise(x):\n",
    "    #print(type(x))\n",
    "    x = array(x)\n",
    "    for k in range(0,2):\n",
    "        #print(\"Iteration \", k)\n",
    "        x_offset = random.choice([0,14])\n",
    "        for i in range(x_offset, x_offset+14):\n",
    "            x[x_offset:x_offset+14,x_offset:x_offset+14] = torch.zeros(x[x_offset:x_offset+14,x_offset:x_offset+14].shape)\n",
    "    x = torch.from_numpy(x)\n",
    "    #print(type(x))\n",
    "    return x\n",
    "            \n",
    "# Apply noise to a set of images\n",
    "def apply_noise(imgs):\n",
    "    noisy_imgs = copy.deepcopy(imgs)\n",
    "            \n",
    "    for i in range(0, len(imgs)):\n",
    "        noisy_imgs[i][0] = noise(imgs[i][0])\n",
    "\n",
    "    imgs = imgs.view(imgs.size(0), -1)\n",
    "    noisy_imgs = noisy_imgs.view(noisy_imgs.size(0), -1)\n",
    "    \n",
    "    return imgs, noisy_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test noise function\n",
    "# arr = [[1 for i in range(28)] for j in range(28)]\n",
    "# arr2 = [[1 for i in range(28)] for j in range(28)]\n",
    "# big = [arr, arr2]\n",
    "# apply_noise(big)\n",
    "# for x in big:\n",
    "#     print(np.matrix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 32\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, z_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        self.encoder = nn.Sequential(\n",
    "                            nn.Linear(28*28, 256),\n",
    "                            nn.ReLU(True),\n",
    "                            nn.Linear(256, 128),\n",
    "                            nn.ReLU(True),\n",
    "                            nn.Linear(128, 64),\n",
    "                            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "                            nn.Linear(64, 128),\n",
    "                            nn.ReLU(True),\n",
    "                            nn.Linear(128, 256),\n",
    "                            nn.ReLU(True),\n",
    "                            nn.Linear(256, 28*28),\n",
    "                            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_x):\n",
    "        encoded = self.encoder(input_x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(z_size)\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "def train():\n",
    "    start_training_time = datetime.datetime.now()\n",
    "    epochs = 20\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for train_data in train_loader:\n",
    "            train_imgs, _ = train_data\n",
    "            train_imgs, train_noisy_imgs = apply_noise(train_imgs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_output = autoencoder(train_noisy_imgs)\n",
    "            loss_func = mse_loss(train_output, train_imgs)\n",
    "            loss_func.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss_func.item()#*train_imgs.size(0)\n",
    "            \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss\n",
    "            ))\n",
    "            \n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for validation_data in validation_loader:\n",
    "                val_imgs, _ = validation_data\n",
    "                val_imgs, val_noisy_imgs = apply_noise(val_imgs)\n",
    "                \n",
    "                val_output = autoencoder(val_noisy_imgs)\n",
    "                loss_func = mse_loss(val_output, val_imgs)\n",
    "                val_loss += loss_func.item()#*val_imgs.size(0)\n",
    "        \n",
    "        val_loss = val_loss/len(validation_loader)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print('Epoch: {} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            val_loss\n",
    "            ))\n",
    "        \n",
    "    end_training_time = datetime.datetime.now()\n",
    "    diff_training_time = end_training_time - start_training_time\n",
    "    print(diff_training_time)\n",
    "    \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(loss_training, loss_type):\n",
    "    if loss_type is \"training\":\n",
    "        plot_label = 'training set MSE loss'\n",
    "    elif loss_type is \"validation\":\n",
    "        plot_label = 'validation set MSE loss'\n",
    "    plt.axis([0, 20, min(loss_training), max(loss_training)])\n",
    "    plt.plot(loss_training, label=plot_label)\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot(train_loss, \"training\")\n",
    "loss_plot(val_loss, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_iter = iter(test_loader)\n",
    "    for i in range(8):\n",
    "        test_imgs, _ = test_iter.next()\n",
    "        test_imgs, test_noisy_imgs = apply_noise(test_imgs)\n",
    "\n",
    "        output = autoencoder(test_noisy_imgs)\n",
    "        test_noisy_imgs = test_noisy_imgs.view(4, 1, 28, 28)\n",
    "        test_noisy_imgs = test_noisy_imgs.detach().numpy()\n",
    "        output = output.view(4, 1, 28, 28)\n",
    "        output = output.detach().numpy()\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(10,4))\n",
    "        for test_noisy_imgs, row in zip([test_noisy_imgs, output], axes):\n",
    "            for img, ax in zip(test_noisy_imgs, row):\n",
    "                ax.imshow(np.squeeze(img), cmap='gray')\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
